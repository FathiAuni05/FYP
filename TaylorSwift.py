import streamlit as st
import subprocess
import sys

# Install required packages
subprocess.check_call([sys.executable, "-m", "pip", "install", "wordcloud", "pandas", "matplotlib"])

pip install -r requirements.txt

import nltk

# Download necessary NLTK resources
nltk.download('vader_lexicon')
nltk.download('stopwords')
nltk.download('wordnet')

! unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/

from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords, wordnet, words

wordnet.synsets('na')

# Import Libraries
import requests
import os
from wordcloud import WordCloud
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# URL of the font file
# Added 'https://' to the URL
font_url = "https://github.com/Phonbopit/sarabun-webfont/raw/master/fonts/thsarabunnew-webfont.ttf"

# File path to save the font file
font_path = "thsarabunnew-webfont.ttf"

# Download the font file
response = requests.get(font_url)
with open(font_path, 'wb') as f:
    f.write(response.content)

text_path = "taylor_swift_lyrics.csv"
# Specify encoding explicitly
df = pd.read_csv(text_path, encoding='latin1')

df.head()

import nltk
import string
from nltk.corpus import stopwords, words, wordnet
from nltk.tokenize import word_tokenize

# Download the necessary NLTK data packages
nltk.download('punkt_tab') # Download the Punkt Tokenizer Models
nltk.download('words')

english_words = set(words.words())

# Preprocess Data
tokens = word_tokenize(" ".join(df['lyric']))
english_stopwords = set(stopwords.words('english'))

# Filter out stopwords, punctuation, and words starting with apostrophe
filtered_tokens = [word.lower() for word in tokens
                   if word.strip()
                   and word.lower() not in english_stopwords
                   and wordnet.synsets(word)
                   and len(word) > 2
                   and word.lower() in english_words]

# Join tokens into a single string
text_for_wordcloud = ' '.join(filtered_tokens)

print(len(filtered_tokens))
print(filtered_tokens[:100])

# Create a pandas Series from the list ðŸ“‘
word_series = pd.Series(filtered_tokens)
word_counts = word_series.value_counts().reset_index()

# Rename the columns for clarity
word_counts.columns = ["Word", "Count"]

# Display the word counts table
word_counts.head(20).style.background_gradient(cmap='YlGn')

# Define colors using a colormap
colors = plt.cm.YlGn(np.linspace(0.8, 0.1, len(word_counts["Word"][:15])))

# Plot the top 15 most frequent words with gradient color
plt.figure(figsize=(12, 8))
bars = plt.bar(word_counts["Word"][:15], word_counts["Count"][:15], color=colors)

# Add color gradient to bars
for bar, color in zip(bars, colors):
    bar.set_color(color)

plt.xlabel("Words", fontsize=14)
plt.ylabel("Frequency", fontsize=14)
plt.title("Top Most Frequent Words", fontsize=16)
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.yticks(fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# Create a WordCloud using the downloaded font
wordcloud = WordCloud(
                      relative_scaling=0.3,
                      min_font_size=1,
                      background_color = "white",
                      width=1024,
                      height=768,
                      max_words=200,
                      colormap='plasma', 
                      scale=3,
                      font_step=4,
                      collocations=False,
                      margin=5
                      ).generate(' '.join(filtered_tokens))

# Generate the WordCloud
wordcloud.generate(text_for_wordcloud)

# Display Word Cloud ðŸ›«
plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

# Initialize the SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()

# Calculate the sentiment scores for the entire text
sentiment_score = analyzer.polarity_scores(text_for_wordcloud)

# Extract the individual scores
neg = sentiment_score['neg']
neu = sentiment_score['neu']
pos = sentiment_score['pos']

sentiment_score

# Create labels for the pie chart
labels = ['Negative', 'Neutral', 'Positive']

# Create sizes for each sentiment category
sizes = [neg, neu, pos]

# Plot the pie chart
plt.figure(figsize=(8, 6))
plt.pie(sizes, labels=labels, colors=plt.cm.Set2.colors, autopct='%1.1f%%', startangle=140)
plt.title('Sentiment Distribution')
plt.axis('equal')
plt.show()

# Initialize list to store word sentiments
word_sentiments = []

# Initialize variables to accumulate sentiment scores
positive_score_sum = 0
negative_score_sum = 0
neutral_score_sum = 0
positive_count = 0
negative_count = 0
neutral_count = 0
total_words = 0
# Count occurrences of each sentiment and accumulate sentiment scores
for word in filtered_tokens:
    total_words +=1
    sentiment_score = analyzer.polarity_scores(word)['compound']
    if sentiment_score >= 0.05:
        word_sentiments.append(('Positive', word))
        positive_count += 1
        positive_score_sum += sentiment_score
    elif sentiment_score <= (-0.05):
        word_sentiments.append(('Negative', word))
        negative_count += 1
        negative_score_sum += sentiment_score
    else:
        word_sentiments.append(('Neutral', word))
        neutral_count += 1
        neutral_score_sum += sentiment_score

# Calculate average sentiment score for each emotion
average_positive_score = positive_score_sum / positive_count if positive_count != 0 else 0
average_negative_score = negative_score_sum / negative_count if negative_count != 0 else 0
average_neutral_score = neutral_score_sum / neutral_count if neutral_count != 0 else 0
average_total_score = (positive_score_sum + negative_score_sum + neutral_score_sum) / total_words

# Print results
print("pos:", positive_count)
print("neg:", negative_count)
print("neu:", neutral_count)
print("Total:", total_words)

# Print results with two decimal places
print("Average pos Score: {:.2f}".format(average_positive_score))
print("Average neg Score: {:.2f}".format(average_negative_score))
print("Average neu Score: {:.2f}".format(average_neutral_score))
print("Average Total Score: {:.2f}".format(average_total_score))

# Create separate lists for each word type
all_words = [word for sentiment, word in word_sentiments]
positive_words = [word for sentiment, word in word_sentiments if sentiment == 'Positive']
negative_words = [word for sentiment, word in word_sentiments if sentiment == 'Negative']
neutral_words = [word for sentiment, word in word_sentiments if sentiment == 'Neutral']

# Count occurrences of each word type
word_counts = pd.Series(all_words).value_counts().head(30)
positive_word_counts = pd.Series(positive_words).value_counts().head(30)
negative_word_counts = pd.Series(negative_words).value_counts().head(30)
neutral_word_counts = pd.Series(neutral_words).value_counts().head(30)

# Create a DataFrame to display the top of each word type
data = {
    'All Words': word_counts.index,
    'All Counts': word_counts.values,
    'Pos Words': positive_word_counts.index,
    'Pos Counts': positive_word_counts.values,
    'Neg Words': negative_word_counts.index,
    'Neg Counts': negative_word_counts.values,
    'Neu Words': neutral_word_counts.index,
    'Neu Counts': neutral_word_counts.values
}

df = pd.DataFrame(data)

# Set column names
df.columns = ["Word", "Count", "Pos Words", "Pos Counts", "Neg Words", "Neg Counts", "Neu Words", "Neu Counts"]

# Display the DataFrame with styled background gradients
styled_df = df.head(30).style.background_gradient(cmap='YlGn')
styled_df

# Generate word clouds for each sentiment type
wordclouds = {}
for sentiment_type in ['Positive', 'Negative', 'Neutral']:
    words = [word[1] for word in word_sentiments if word[0] == sentiment_type]
    if words:
        wordcloud = WordCloud(relative_scaling=0.3,
                              min_font_size=1,
                              background_color="white",
                              width=1024,
                              height=768,
                              max_words=500,
                              colormap='plasma',
                              scale=3,
                              font_step=4,
                              collocations=False,
                              margin=5).generate(' '.join(words))
        wordclouds[sentiment_type] = wordcloud

# Display the word clouds  
plt.figure(figsize=(18, 6))
for i, sentiment_type in enumerate(['Positive', 'Negative', 'Neutral']):
    if sentiment_type in wordclouds:
        plt.subplot(1, 3, i+1)
        plt.imshow(wordclouds[sentiment_type], interpolation='bilinear')
        plt.title(sentiment_type + ' Words')
        plt.axis('off')
plt.show()

!pip install NRCLex==3.0.0

from nrclex import NRCLex

text_object = NRCLex(' '.join(filtered_tokens))
text_object.affect_frequencies

text_object.top_emotions

# Get the total number of words
total_words = len(text_object.words)
print("Total words in the text:", total_words)

sentiment_scores = pd.DataFrame(list(text_object.raw_emotion_scores.items()))
sentiment_scores = sentiment_scores.rename(columns={0: "Sentiment", 1: "Count"})
sentiment_scores.style.background_gradient(cmap='YlGn')

# Plot the pie chart
plt.figure(figsize=(8, 6))
plt.pie(sentiment_scores['Count'], labels=sentiment_scores['Sentiment'], colors=plt.cm.tab20c.colors, autopct='%1.1f%%', startangle=140)
plt.title('Sentiment Distribution')
plt.axis('equal')
plt.show()

from collections import Counter

# Define color maps for each sentiment
sentiment_cmaps = {
    'fear': 'viridis',
    'anger': 'magma',
    'anticipation': 'viridis',
    'trust': 'cividis',
    'surprise': 'cool',
    'positive': 'viridis',
    'negative': 'inferno',
    'sadness': 'inferno',
    'disgust': 'copper',
    'joy': 'spring'
}

# Get all unique sentiments in the text
unique_sentiments = list(set(sentiment for emotions in text_object.affect_dict.values() for sentiment in emotions))

# Create a pandas Series from the list of filtered tokens
word_series = pd.Series(filtered_tokens)

# Create a dictionary to store word frequencies for each sentiment
sentiment_word_counts = {}

# Calculate word frequencies for each sentiment
for sentiment in unique_sentiments:
    words_for_sentiment = [word for word, emotions in text_object.affect_dict.items() if sentiment in emotions]
    sentiment_word_counts[sentiment] = word_series[word_series.isin(words_for_sentiment)].value_counts().head(10)

# Plotting the bar graph for each sentiment
num_rows = len(unique_sentiments) // 2 + (1 if len(unique_sentiments) % 2 != 0 else 0)
num_cols = 2
fig, axs = plt.subplots(num_rows, num_cols, figsize=(12, num_rows * 6))
axs = axs.ravel()

for i, sentiment in enumerate(unique_sentiments):
    word_counts = sentiment_word_counts[sentiment]
    axs[i].barh(word_counts.index, word_counts.values, color=plt.cm.YlGn(word_counts.values / max(word_counts.values)), alpha=0.9)
    axs[i].set_title(f'Top 10 Words for {sentiment.capitalize()} Sentiment')
    axs[i].set_xlabel('Frequency')
    axs[i].tick_params(axis='y')
    axs[i].set_yticklabels(word_counts.index)
    axs[i].invert_yaxis()

# Hide any remaining empty subplots
for j in range(len(unique_sentiments), num_rows * num_cols):
    axs[j].axis('off')

plt.tight_layout()
plt.show()

# Determine the number of rows and columns needed
num_rows = len(unique_sentiments) // 3 + (1 if len(unique_sentiments) % 3 != 0 else 0)
num_cols = 3

# Create subplots with 3 columns
fig, axs = plt.subplots(num_rows, num_cols, figsize=(20, num_rows * 6))

# Flatten the axis array
axs = np.ravel(axs)

# Calculate word frequencies for each sentiment
for sentiment in unique_sentiments:
    words_for_sentiment = [word for word, emotions in text_object.affect_dict.items() if sentiment in emotions]
    sentiment_word_counts[sentiment] = word_series[word_series.isin(words_for_sentiment)].value_counts()

# Create a WordCloud for each sentiment
for i, sentiment in enumerate(unique_sentiments):
    words_for_sentiment = [word for word, emotions in text_object.affect_dict.items() if sentiment in emotions]
    word_counts = sentiment_word_counts[sentiment]

    # Create a WordCloud
    wordcloud = WordCloud(
        relative_scaling=0.3,
        min_font_size=1,
        background_color="white",
        width=400,
        height=300,
        max_words=500,
        colormap=sentiment_cmaps.get(sentiment),
        scale=3,
        font_step=4,
        collocations=False,
        margin=5
    )

    # Generate WordCloud from word frequencies
    wordcloud.generate_from_frequencies(word_counts.to_dict())

    # Display the WordCloud in the appropriate subplot
    axs[i].imshow(wordcloud, interpolation='bilinear')
    axs[i].set_title(f"Word Cloud for {sentiment.capitalize()} Sentiment")
    axs[i].axis('off')

    # Hide any remaining empty subplots
for j in range(len(unique_sentiments), num_rows * num_cols):
    axs[j].axis('off')

# Adjust layout
plt.tight_layout()
plt.show()
